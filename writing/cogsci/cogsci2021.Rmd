---
title: "Children know what words other children know"
bibliography: kidaoa.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: >
    \author{Ashley Leung \\
         \texttt{ashleyleung@uchicago.edu} \\
        Department of Psychology \\ University of Chicago
    \And \textbf{Benjamin C. Morris} \\
         \texttt{benmorris@uchicago.edu} \\
        Department of Psychology \\ University of Chicago
    \And Daniel Yurovsky \\
         \texttt{yurovsky@cmu.edu} \\
        Department of Psychology \\ Carnegie Mellon University}

abstract: > 
    To communicate successfully, we need to use words that our conversational partner understands. Adults maintain precise models of the words people are likely to know, using both prior experience with their conversational partner and general metalinguistic information. Do children also know what words others are likely to know? We asked children ages 4-8 ($n =$ 62) to predict whether a very young child would know each of 15 familiar animal words, all typically acquired within a 6-month range. With minimal information, even children as young as 4 made reliable predictions about the target child's vocabulary knowledge. Further, children's accuracies improved significantly over the course of development. Thus, even preschool age children are adept at inferring other children's vocabulary knowledge, and they could leverage this information to communicate effectively.
    
keywords: >
    communication, metalinguistic, knowledge reasoning, cognitive development
    
output: cogsci2016::cogsci_paper
final-submission:  \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo = F, warning = F, cache = T, 
                      message = F, sanitize = T)
# Note: to build, 
options(digits=2)
```

```{r}
library(tidyverse)
library(here)
library(ggrepel)
library(lme4)
library(ggthemes)
library(broom)
library(broom.mixed)
library(effectsize)
library(papaja)
library(kableExtra)
library(lmerTest)
library(png)
library(xtable)
library(tidyboot)

theme_set(theme_few(base_size = 12))
```


# Introduction


Imagine visiting the zoo with your friend and their 2-year-old. As you walk by the peacocks, you hear your friend say, "Do you see those blue birds?" Immediately, you know that your friend is talking to their child and not you. If they were talking to you, saying “peacock” would be perfectly clear; however, "blue bird" might be a better description for a child who has never seen a peacock before. Even when talking about the same object, we use different words depending on what we think our conversational partners know and don't know. 

Adults track and adapt to their conversational partners' knowledge with relative ease. For example, adults reduce the information they give when re-telling a story to someone who has heard it before, but not when telling the story to a new partner [@galati2010]. Adults can adapt even to partners who are quite different from them, as in the case of parents and their children. Parents model the fine-grained details of their children's vocabularies and use these models in spontaneous communication [e.g., using "blue bird" to describe a peacock; @leung2021]. These vocabulary models are shaped by extensive individual parent-child interactions, but likely also general metalinguistic knowledge-- for instance, that shorter words are typically simpler than longer words and more likely to be known. 

In line with this conjecture, @kuperman2012 asked people to report the age at which they understood a set of 30,000 English words. They found that adults typically overestimated the ages at which words are learned, but were quite accurate at judging their relative order. Thus, even adults without children have access to the kind of metalinguistic information they could use to calibrate speech to children's knowledge.

Can children use this same kind of information to predict what words others know? Reasoning about another person's specific lexical knowledge could be difficult for young children. Children often over-attribute knowledge to others, especially knowledge they themselves already have [@birch2003; @ghrear2020]. This bias to over-attribute knowledge could hinder children's ability to reason about a younger child's knowledge.  

However, even preschool age children can make non-egocentric knowledge judgments in some tasks. Asked about variety of general knowledge skills, young children attribute different levels of knowledge to infants, preschool children, and adults [@fitneva2010; @taylor1991]. While these studies sometimes include vocabulary items [e.g., @taylor1991], they test whether children make broad distinctions between different people's knowledge, such as an infant not knowing any words, a child knowing simple words (e.g., *happy*), and an adult knowing complex words (e.g., *hypochondriac*). 

We ask whether children can infer another child's specific vocabulary knowledge to make word-level predictions consistent with normative age of acquisition. One study suggests that children as young as 5 can accurately estimate the age and order in which they learned a variety of words [@walley1992], but can they reason about *other* children's vocabulary knowledge? We introduced 4- to 8-year-old children to a younger fictional child, and asked them to make judgments about the target child's knowledge of various familiar words. Even 4-year-old children made judgments that matched the estimated order of acquisition, such that they judged the fictional child to be more likely to know earlier-acquired words and less likely to know later-acquired words. Older children's judgments more reliably recovered the order of acquisition. We end by discussing children's own explanations for why the target child would know (or not know) particular words.

# Method

```{r}
kidaoa_data <- read_csv(here("clean_data/kidaoa_clean_with_adults.csv")) %>% select(-X1) %>%
    mutate(age = if_else(age == "adult", "Adults", age)) 

secondary_kidaoa_data <- read_csv(here("clean_data/supp_kid_data.csv"))

kid_n <- kidaoa_data %>% filter(age != "Adults") %>% distinct(id) %>% nrow(.)

explanations <- read_csv(here("kid_data/explanations_coded_AL.csv"))

```

## Stimuli

To create a coherent game that would be enjoyable for children, we selected stimuli from a single domain (animals). Our stimuli consisted of 15 animal words, along with corresponding images of each animal. We pulled all animal images ($n =$ 45) from a normed image set [@rossion2004; recoloring of @snodgrass1980]. To ensure our stimuli spanned a range of ages of acquisition (AoAs), we ranked the animal words from earliest to latest AoA, using adult estimates from @kuperman2012, and split the words into five bins. In order to select animal images that are recognizable and typically identified by a single name, we chose the three animals from each AoA bin with the highest naming agreement according to a naming task with children [@cycowicz1997]. 

The resulting animal words, ordered by estimated AoA, were *dog*, *duck*, *cat*, *pig*, *fish*, *turtle*, *zebra*, *elephant*, *snake*, *penguin*, *gorilla*, *owl*, *raccoon*, *leopard*, and *lobster.* Although adult AoA estimates for these words range from 2.5 to 7.5 years old [@kuperman2012], all of these animal words are generally acquired by age 3 according to parent-reported estimates of children's vocabulary knowledge [@frank2017]. Because the youngest children in our study were 4 years old, we expected all participants to know these animal words.

## Participants

We pre-registered a planned sample of 60 children ages 4-8, with 12 children per year-wise age group. Due to overrecruitment, our final sample included 62 children (12 4-year-olds, 13 5-year-olds, 13 6-year-olds, 12 7-year-olds, 12 8-year-olds). Based on a pre-registered exclusion criterion, children who failed to answer all of the questions were excluded and replaced (an additional 6 children). Families were recruited online, primarily through a US University database of families who have expressed interest in doing research or previously participated. Children completed this study over Zoom, interacting with a live experimenter who navigated a slide-style, animated Qualtrics survey.

A separate sample of 30 adults were recruited via Amazon Mechanical Turk. The adult sample provides a simple test that our task elicits robust inferences about the target child's lexical knowledge, and that these inferences correspond to extant AoA data. Adult participants completed the same task using Qualtrics, with minor modifications as described below.


## Procedure

```{r task-method, fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "The structure of an example trial. The experimenter labeled the animal, then asked the child “Do you think Sam knows that this is called an elephant?” Based on their response, children were then asked to provide a confidence judgment on a 3-point scale (a little sure, medium sure, very sure)."}
img <- png::readPNG(("figs/task-method.png"))
grid::grid.raster(img)
```

*Introduction.* Children were shown a picture of a child named "Sam" (seen in Figure \ref{fig:task-method}). Children were anchored to Sam's knowledge of various familiar skills, specifically some skills that Sam has acquired (e.g., coloring), and some that Sam has not yet acquired (e.g., reading). Children were then specifically anchored to Sam’s possible word knowledge in a non-animal domain. They were given an example of one word Sam knows (*car*), and one word that Sam does not know (*piano*). This introduction was intended to ensure that children understand there are things Sam does *not* know yet (even things children themselves likely know, such as how to read).

*Trial structure.* On each trial, children were shown a drawing of a familiar object or animal [@rossion2004]. The experimenter first labeled the object (e.g., “Look, it's [an elephant]!"), and then asked about the target child's knowledge (e.g., "Do you think Sam knows that this is called [an elephant]? Yes or no?”). Based on their response, children were then asked a follow-up question: "How sure are you that Sam [knows/doesn't know] that this is called [an elephant]--a little sure, medium sure, or very sure?" All questions were presented with accompanying pictures of thumbs [up/down] of varying size (see Figure \ref{fig:task-method}). This trial structure allowed us to collect a gradient response while maintaining simplicity, as young children may struggle to comprehend a 6-point scale. Children as young as 3 are able to engage in uncertainty monitoring and report their confidence, although these skills do develop in the preschool years [@lyons2011]. Children's responses to these two items were recoded onto a 1-6 scale from 1--*very sure Sam doesn't know* to 6--*very sure Sam knows* (Figure \ref{fig:task-method}). 

The experimenter provided no evaluative feedback on any trials, but did offer consistent neutral feedback (e.g., repeating the child's answer or saying "Okay!"). When a child failed to respond within about 5 seconds or offered a non-canonical response (e.g., saying "Maybe"), the experimenter acknowledged the child's answer and then repeated the question with the possible responses. If a child did not answer after the question was repeated, the experimenter moved on and marked the trial as no response. These were considered "incomplete" sessions and these participants were not included in the final sample.

*Familiarization trials.* Children first completed two non-animal familiarization trials, one for an early-acquired word (*ball*) and one for a late-acquired word (*artichoke*). These trials followed the trial structure described above and were intended to help familiarize children with the structure of the questions and scales. These trials were always asked first and in a fixed order.

*Animal trials.* Children were then shown 15 trials of the same form (see example trial in Figure \ref{fig:task-method}). For the 15 animal trials, trial order was randomized across participants to control for any potential order effects in children's responses.

*Explanation.* After completing the final animal trial, children were asked an open-ended explanation question about their final judgment (e.g., "Why do you think Sam [knows/doesn't know] that this is called [an elephant]?"). Because the trial order was randomized, the explanations concerned different animal words across participants.

*Final check questions.* Children were asked two questions about Sam's skill knowledge, one early-acquired skill (*going up and down stairs*) and one very late-acquired skill (*driving a car*). These questions again followed the general trial structure described above. The skill knowledge items were included as an additional check that children at all ages were able to use the scale appropriately, in case young children failed to differentiate animal words based on AoA. Lastly, children were asked to report how old they thought Sam was. This question was intended to assess another aspect of children's belief about Sam. Sam's photo and skill knowledge were intended to indicate toddlerhood.

#### Adult procedure.

Adult participants completed a minimally adapted version of the same task online via Qualtrics. Unlike children, adults were simply presented with the full 6-point scale (1--*very sure Sam doesn't know* to 6--*very sure Sam does know*). Additionally, the task was administered asynchronously, so adult participants did not interact with an experimenter or receive any feedback during the task. Otherwise, the adult task was identical to the child task described above. 

```{r overall, fig.width = 3.5, fig.cap = "Comparing adult AoA estimates (in years, taken from Kuperman et al., 2012) and children’s judgments on our 6-point scale (1 = very sure Sam doesn’t know; 6 = very sure Sam knows). The black lines show 95\\% confidence intervals for each item. The shaded region shows the confidence interval based on a linear regression estimated from the raw data."}
booted_kid_overall <- kidaoa_data %>% 
  # drop the adults, so it's only kids performance average
  filter(age != "Adults") %>%
  group_by(word, aoa) %>%
  tidyboot_mean(judgment)

#use the raw data to make line and variability
   #points show averages
overall <- ggplot(kidaoa_data, aes(x = aoa, y = judgment,
                      label = word)) + 
  # geom_jitter() +
  geom_smooth(method = "lm", fill = "lightblue") +
  geom_linerange(data = booted_kid_overall, aes(y = empirical_stat, ymin = ci_lower, ymax = ci_upper)) +
  geom_label(data = booted_kid_overall, size = 2.3, label.size = .1, label.padding = unit(.13, "lines"), aes(y = empirical_stat)) +
  xlab("Adult AoA Estimates") +
  ylab("Children's Judgments") +
  scale_y_continuous(breaks = c(1:6), limit = c(1,6))

print(overall, type = "figure", comment = F, fig.placement = "b", floating = TRUE)
```



```{r development, fig.env = "figure*", fig.width = 7, fig.height = 3, fig.cap = "Children's and adults' judgements about the target child's word knowledge across development, compared with adult AoA estimates (in years, taken from Kuperman et al., 2012). Each point represents 1 of the 15 word items, with black lines showing 95\\% percent confidence intervals for each item. The shaded region shows the confidence interval based on a linear regression estimated from the raw data."}
boot_by_age <- kidaoa_data %>% 
  # mutate(word = substr(word, 0, 3)) %>%
  group_by(age, word, aoa) %>%
  tidyboot_mean(judgment)

labels <- boot_by_age %>%
  filter(word %in% c("cat", "penguin"))

#use the raw data to make line and variability
   #points show averages by age
ggplot(kidaoa_data, aes(x = aoa, y = judgment, group = age,
                      label = word)) + 
  # # if we want to add raw data
  # geom_jitter(color = "grey" , alpha = .5) +
  geom_smooth(method = "lm", fill = "lightblue") +
  geom_linerange(data = boot_by_age, color = "black", alpha = .4,
                  aes(y=empirical_stat, ymin = ci_lower, ymax= ci_upper)) +
  geom_point(data = boot_by_age, size = 1.5, color = "black", alpha = .75,
                  aes(y=empirical_stat)) +
  # a bit hard to see on this plot if we use labels, suggest we use regular points instead
  geom_label(data = labels, size = 2.3, label.size = .1, label.padding = unit(.13, "lines"), color = "black",
                  aes(y=empirical_stat)) +
  facet_grid(.~age) + 
  xlab("Adult AoA Estimates") +
  ylab("Knowledge Judgments") +
  coord_cartesian(ylim = c(0,7)) +
  scale_y_continuous(breaks = c(1:6))
```



```{r age_terms, fig.cap = "Coefficient estimates of the effect of age of acquisition on children's and adults' knowledge judgments. Points indicate means, error bars indicate 1 standard deviation.", fig.height = 2.5}
age_terms <- kidaoa_data %>%
  group_by(age) %>%
  nest() %>%
  mutate(model = map(data, ~lmer(judgment ~ aoa + (1 | id) + (1 | word), 
                                 data = .x) %>% tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(term == "aoa") %>%
  mutate(type = if_else(age == "Adults", "Adult", "child"),
         age_numeric = as.numeric(age),
         age_numeric = if_else(is.na(age_numeric), 9, age_numeric))

age_term_annotations <- tibble(age_numeric = c(5, 9),
                               estimate = -.35, -.35,
                               plot_label = c("Children", "Adults"),
                               type = c("child", "Adult"))

ggplot(age_terms, aes(x = age_numeric, y = estimate, color = type,
                      group = type)) + 
  geom_pointrange(aes(ymin = estimate - std.error, ymax = estimate + 
                        std.error)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme(legend.position = "none") +
  geom_text(aes(label = plot_label), data = age_term_annotations) +
  scale_color_ptol() +
  labs(x = "Age (years)", y = "AoA's effect on judgments") +
  coord_cartesian(ylim = c(-1, 0)) +
  scale_x_continuous(breaks = 4:9, labels = c("4", "5", "6", "7", "8", "adult"),
                     limits = c(3.5,9.5))
  
```


# Results

## Familiarization trials

Two familiarization items (*ball* and *artichoke*) were included to help children get accustomed to the general trial structure. We report children's responses on these familiarization items here. We used a mixed effects model using the \texttt{lme4} package in \texttt{R} [@bates2015], predicting children's knowledge judgments from the item with a random effect of participant.

```{r}
#familiarization trials
training_data <- secondary_kidaoa_data %>% filter(word %in% c('ball', 'artichoke'))
training_age_average <- training_data %>% 
  group_by(age, word) %>% 
  tidyboot_mean(judgment)

training_overall_average <- training_data %>% 
  group_by(word) %>% 
  tidyboot_mean(judgment)
ball <- training_overall_average %>% filter(word == "ball") %>% pull(empirical_stat)
artichoke <- training_overall_average %>% filter(word == "artichoke") %>% pull(empirical_stat)

#model, overall
training_overall <- lmer(judgment ~ word + (1 | id), data = training_data) %>% tidy() %>%
  mutate(p.value = printp(p.value, digits = 2)) %>%
  filter(term == "wordball")

#model, by age
training_4s <- lmer(judgment ~ word + (1 | id),
       data = training_data %>% filter(age == 4)) %>% tidy() %>%
   mutate(p.value = printp(p.value, digits = 2)) %>%
  filter(term == "wordball")

training_5s <- lmer(judgment ~ word + (1 | id),
       data = training_data %>% filter(age == 5)) %>% tidy() %>%
   mutate(p.value = printp(p.value, digits = 2)) %>%
  filter(term == "wordball")
```

```{r}
#skills trials
skills_data <- secondary_kidaoa_data %>% filter(word %in% c('stairs', 'car'))

skills_overall_average <- skills_data %>% 
  group_by(word) %>% 
  tidyboot_mean(judgment)
car <- skills_overall_average %>% filter(word == "car") %>% pull(empirical_stat)
stairs <- skills_overall_average %>% filter(word == "stairs") %>% pull(empirical_stat)

#model, overall
skills_overall <- lmer(judgment ~ word + (1 | id), data = skills_data) %>% tidy() %>%
  mutate(p.value = printp(p.value, digits = 2)) %>%
  filter(term == "wordstairs")

#model, by age
skills_4s <- lmer(judgment ~ word + (1 | id),
       data = skills_data %>% filter(age == 4)) %>% tidy() %>%
   mutate(p.value = printp(p.value, digits = 2)) %>%
  filter(term == "wordstairs")
```

Overall, children were significantly more likely to report that Sam knows the word *ball* ($mean =$ `r ball`) than that Sam knows the word *artichoke* ($mean =$ `r artichoke`, $\beta =$ `r training_overall$estimate`, $t =$ `r training_overall$statistic`, $p$ `r training_overall$p.value`). Analyzing judgments separately for each age group, 4-year-olds did not significantly differentiate the two familiarization items ($\beta =$ `r training_4s$estimate`, $t =$ `r training_4s$statistic`, $p =$ `r training_4s$p.value`). All other age groups significantly differentiated the two familiarization items ($ps <$ 0.05).


## Skill knowledge

As an initial check that children at all ages were able to use the scale appropriately and infer knowledge in an easier case, we included two questions about the target child's skill knowledge. Note that the two skill items (*going up and down stairs* and *driving a car*) are in line with children's own knowledge. That is, children should be able to answer these questions appropriately even if they are reasoning egocentrically about their own knowledge.

Overall, children differentiated the target child's skill knowledge on these two items. We used a similar mixed effects structure predicting children's knowledge judgments from the item with a random effect of participant. Children were significantly more likely to report that the target child knows how to go up and down stairs ($mean =$ `r stairs`) than that the child knows how to drive a car ($mean =$ `r car`, $\beta =$ `r skills_overall$estimate`, $t =$ `r skills_overall$statistic`, $p$ `r skills_overall$p.value`). Analyzing judgments separately for each age group, even 4-year-olds significantly differentiated the two skill items ($\beta =$ `r skills_4s$estimate`, $t =$ `r skills_4s$statistic`, $p$ `r skills_4s$p.value`).

## Judgments of vocabulary knowledge

Our primary analyses compare knowledge judgments on our 6-point scale to AoA estimates from adults [taken from @kuperman2012]. Data were analyzed using a pre-registered mixed effects model. We predicted knowledge judgments from adult AoA estimates, including random effects for participant and word.

We expected that overall, children's judgments would recover the ordinal shape of age of acquisition data for these items. That is, children would infer that the target child is most likely to know early-acquired words, and least likely to know late-acquired words. As a result, we expected a negative relationship between judgments of the target child's lexical knowledge and adult AoA estimates. 


```{r model}
model_data <- kidaoa_data %>%
  filter(age != "Adults") %>%
  ungroup() %>%
  mutate(age = scale(as.numeric(age), scale = F),
         aoa_std = scale(aoa),
         judgment_std = scale(judgment))

overall_model <- model_data %>%
  lmer(judgment ~ aoa + (1 | id) + (1|word),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(d = t_to_d(statistic, df)$d,
         d_low = t_to_d(statistic, df)$CI_low,
         d_high = t_to_d(statistic, df)$CI_high) %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))

overall_aoa <- overall_model %>% filter(term == "aoa")
```

```{r}
#adult responses
adults <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == "Adults"))) %>% tidy()

adults_aoa <- adults %>% filter(term == "aoa") %>% mutate(p.value = printp(p.value, digits = 2))
```

First, analyzing adults responses on our task, we saw the predicted negative correlation between AoA and adults' judgments of the target child's knowledge (Figure \ref{fig:development}, $\beta =$ `r adults_aoa$estimate`, $t =$ `r adults_aoa$statistic`, $p$ `r adults_aoa$p.value`). This confirmed that our task elicited reliable predictions from adults, and that adults' inferences about the target child's knowledge match predictions from AoA estimation tasks [@kuperman2012].


```{r}
dev_model <- model_data %>%
  lmer(judgment ~ aoa * age + (1 | id) + (1|word),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(d = t_to_d(statistic, df)$d,
         d_low = t_to_d(statistic, df)$CI_low,
         d_high = t_to_d(statistic, df)$CI_high) %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))

dev_aoa <- dev_model %>% filter(term == "aoa")
dev_age <- dev_model %>% filter(term == "age")
dev_interaction <- dev_model %>% filter(term == "aoa:age")
```

Do children's judgments about another child's vocabulary knowledge also reflect a sensitivity to which words are learned earlier or later? Overall, we found a significant negative correlation between AoA and children's judgments ($\beta =$ `r overall_aoa$estimate`, $t =$ `r overall_aoa$statistic`, $p$ `r overall_aoa$p.value`). As a group, children were more confident that the target child would know an early-acquired word (e.g., *dog*), and also more confident that the target child would *not* know a late-acquired word <!--children judged that the target child would be most likely to know an early-acquired word (e.g., *dog*) and least likely to know a late-acquired word-->(e.g., *lobster*, see Figure \ref{fig:overall}).

We then asked whether children develop sensitivity to Sam's vocabulary knowledge, with older children's judgments recovering word-level AoA data more closely. We used the same mixed effects model but included an effect of age and an interaction between AoA and age. We again found a reliable main effect of AoA ($\beta =$ `r dev_aoa$estimate`, $t =$ `r dev_aoa$statistic`, $p$ `r dev_aoa$p.value`), a main effect of age ($\beta =$ `r dev_age$estimate`, $t =$ `r dev_age$statistic`, $p$ `r dev_age$p.value`) and a significant interaction between the two ($\beta =$ `r dev_interaction$estimate`, $t =$ `r dev_interaction$statistic`, $p$ `r dev_interaction$p.value`). As predicted, older children's judgments were more adult-like, such that they more robustly reflected adult estimates of the order of acquisition <!--As predicted, we observed that older children judged whether Sam would know each animal in a more adult-like, and more accurate, way--> (Figure \ref{fig:development}).

```{r}
# effect within each age group
fours <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == 4))) %>% tidy()
fives <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == 5))) %>% tidy()
sixes <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == 6)))
sevens <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == 7)))
eights <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == 8))) %>% tidy()
adults <- (lmer(judgment ~ aoa + (1 | id) + (1 | word), data = kidaoa_data %>% filter(age == "Adults"))) %>% tidy()

#fours are weakest, so we'll report that as such
fours_aoa <- fours %>% filter(term == "aoa") %>% mutate(p.value = printp(p.value, digits = 2))
adults_aoa <- adults %>% filter(term == "aoa") %>% mutate(p.value = printp(p.value, digits = 2))
```

To test the robustness of children's intuition at each age, we ran the model separately for each pre-determined year-wise age group (Figure \ref{fig:age_terms}). We found a significant negative correlation between AoA and children's judgments at all age groups (with the smallest effect in 4-year-olds: $\beta =$ `r fours_aoa$estimate`, $t =$ `r fours_aoa$statistic`, $p =$ `r fours_aoa$p.value`). That is, even 4-year-old children judged that late-acquired animal words were less likely to be known by the target child. Interestingly, judgments from the older two age groups of children were more closely correlated to data from @kuperman2012 than were adult participants' judgments (Figure \ref{fig:age_terms}). This appeared to be primarily driven by a greater willingness to judge Sam as moderately or very unlikely to know late-learned animal words, whereas adults were less sure about these same judgments (Figure \ref{fig:development}). We return to this finding in the Discussion.

```{r}
# inferred age
inferred_age <- secondary_kidaoa_data %>% filter(word %in% c('infAge')) %>% filter(!is.na(judgment))

kid_inf_age <- inferred_age %>% summarize(judgment = median(judgment))
```

## Target child age

At the end of the study, participants were asked to guess the target child's age. While the familiarization phase included information about the child's language and skill knowledge, no age was explicitly given. Looking at children's responses, the median response was that the target child was 3 years old. Looking at adult's responses, the median response was that the target child was 4 years old. 

## Explanations

```{r}
#descriptive stats for explanations
exp <- explanations %>%
  select(-c(ID, why)) 

exp[is.na(exp)] <- 0

prop <- exp %>%
  summarise_all(mean)

ages <- kidaoa_data %>%
  filter(age != "Adults") %>%
  select(age, db_id) %>%
  distinct(db_id, .keep_all = T) %>%
  rename(ID = db_id,
         kid_age = age) %>%
  merge(explanations, by = "ID") %>%
  select(-why) %>%
  mutate(group = if_else(kid_age < 6, "Younger", "Older"))

ages[is.na(ages)] <- 0

split_age <- ages %>%
  group_by(group) %>%
  summarise_at(c("age", "experience", "location", "language", "unsure", "other"), mean)

each_age <- ages %>%
  group_by(kid_age) %>%
  summarise_at(c("age", "experience", "location", "language", "unsure", "other"), mean)

```

As an exploratory analysis, we examined the reasons children gave for why the target child would or would not know a given word. While children sometimes offered spontaneous explanations throughout the study, our analysis focused on the explanations elicited after the final animal trial. The explanations were divided into 6 categories: *Language*, *Experience*, *Location*, *Age*, *Unsure*, and *Other*. 

*Language* includes explanations that explicitly appealed to language properties. *Experience* includes explanations that appealed to real-world experience with the referent. *Location* includes explanations that specifically referenced a particular place the animal is associated with. *Age* includes explanations that referenced a particular age or general age group. Any child that failed to answer the explanation question or expressed ignorance was coded as giving an explanation of *Unsure*. An explanation that didn't fall into any of the above categories was coded as *Other*. Note that coding was not mutually-exclusive, so explanations could be coded as including multiple categories. See Table \ref{tab:explanations_table} for examples of each category. Figure \ref{fig:explanations} shows the proportion of children who gave each type of explanation. 


```{r explanations, fig.width = 2.75, fig.height = 2.75, fig.align = "center", fig.cap = "Children's explanations for why they think the target child knew or didn't know an animal word. Categories are not mutually exclusive."}
prop %>%
  pivot_longer(c(age, experience, language, location, unsure, other), 
               names_to = "exp") %>%
  ggplot(aes(x = reorder(exp, value), y = value, fill = exp)) + 
  geom_bar(stat = "identity") +
  theme(legend.position = "none", axis.text.x = element_text(angle = 30, hjust=1)) +
  labs(x = "Explanation Type", y = "Proportion") +
  scale_fill_brewer(palette = "Dark2")
```

To understand how children's explanations may change over development, we divided participants into older (6-8 years old) and younger children (4-5 years old). Unsurprisingly, *Unsure* explanations were much more common in younger children (`r split_age$unsure[2]*100`%) when compared to older children (`r split_age$unsure[1]*100`%). *Language* explanations were used by the highest proportion of children overall (`r prop$language*100`%). Do older children account for all of those explanations? Although more of the older children appealed to *Language* explanations (`r split_age$language[1]*100`%), these explanations were also common in younger children (`r split_age$language[2]*100`%). Thus, while young children were more likely to offer no explanation, the explanations they did offer seemed to rely on factors similar to older children's explanations.


```{r explanations_table, results="asis", tab.env = "table"}
tab <- tibble(Category = c("Language", " ", "Experience", " ", "Location", " ", "Age", " ", "Unsure", " ", "Other", " "),

              `Example Utterance` = c("Because it was a very long word.",
                            "Because it only has 3 letters.",
                            "Because maybe he has a dog.", 
                            "Because gorillas are really rare animals",
                            "Because penguins live in the arctic and it's too cold for little kids...",
                            "Because fish swim under the ocean.",
                            "Because I think I knew that when I was around 3...",
                            "Because if he went to preschool then he probably knew it...",
                            "I don't know.",
                            "I'm not sure.",
                            "Because it had a longer beak than a bird.",
                            "Because it's small.")) %>% 
  xtable(display = c("s", "d", "f"),
         caption = "Example explanations from child participants for each of the five categories used for coding.",
         label = "tab:explanations_table")

print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table*",
      include.rownames = FALSE)
```


# Discussion

Our ability to infer other people's knowledge is crucial for successful communication. Young children are capable of inferring others' general knowledge, but are they also sensitive to another person's *specific* knowledge? We asked 4- to 8-year-old children to estimate another child's knowledge of words, and found that children as young as 4 are sensitive to a younger child's vocabulary knowledge.

Our findings highlight that young children have robust metalinguistic knowledge [@walley1992], and can use that knowledge to make highly specific inferences about other people's vocabularies. The animal words used in our study are generally learned within a 6-month period, yet young children still distinguished earlier-acquired words from later-acquired words in this set. Prior studies have shown that children are sensitive to broad differences in vocabulary knowledge of infants, young children, and adults [@taylor1991]. Our study further demonstrates that children readily make specific, word-level predictions about the language knowledge of another child.

Surprisingly, children's judgments of another child's knowledge did not just approach adults' judgments over development. Compared to adults, the oldest children in our study gave *more* accurate judgments as measured by their correlation with an external measure of age of acquisition [@kuperman2012]. While 7- and 8-year-olds and adults were both highly confident that a young child would know early-acquired animals, adults were less confident that a young child would not know the later-acquired animals. One possibility is that children are more sensitive to differences in the degree of difficulty of these later-acquired animal words because they still remember learning them [see also @walley1992]. In principle one might expect that the youngest children would thus be even more accurate. However, they may have less access to the kinds of meta-linguistic knowledge that older children and adults likely recruited. Alternatively, adults' caution in asserting that the target child did not know the later-acquired animals could have reflected their difficulty in estimating the child's age. In line with this account, adults judged Sam to be a year older than children did on average. In future work we plan to explore these possibilities by asking children and adults to make similar judgments about children of multiple ages.

How are children in our study making estimates about other people's knowledge? Children's own explanations suggest that they use various cues to make their estimates. Overall, language-related explanations were most common, and even preschool age children appealed to this explanation. However, such explanations are difficult to interpret, and the mechanisms underlying children's knowledge estimates are outside the scope of the current study. Future work should more directly probe the features underlying this inference-- to see if children are relying on their own uncertainty, word length (and other linguistic cues), features of the referent itself, or other features.

The current work lays the foundation for future research on how children leverage their knowledge of other people to communicate successfully. While some studies have found that young children struggle in a variety of communicative tasks [e.g. @krauss1977], other work has shown that by age 5, children selectively talk about general or specific characteristics of an object based on their partner's knowledge state [@baer2018]. Why might children struggle in some situations and not others? Our work can begin to address this question by mapping out whether communicative difficulties stem from tracking an interlocutor's knowledge, or problems using that information in language production. Young children eventually become effective communicators, and our work demonstrates that by age 4, children may have one key ability in place: inferring others' specific vocabulary knowledge.


\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Pre-registrations, stimuli, data, and analysis code are available at https://osf.io/hw9r6/.}}

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to DY.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
